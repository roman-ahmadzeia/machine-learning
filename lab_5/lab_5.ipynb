{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Matches For A Dating Site\n",
    "### Roman Ahmad Zeia\n",
    "\n",
    "### Learning Outcomes\n",
    "### We will be choosing different models (at least 3 models), evaluation methodologies  (e.g., cross-validation), performance metrics, and perform model selection, before evaluating the model on the test data set.\n",
    "\n",
    "### We will be building 3 different models (Decision Tree, Nearest Neighbour and Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, make_scorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FrequentFlyerMiles</th>\n",
       "      <th>VideoGameTime</th>\n",
       "      <th>IceCreamConsumed</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40920</td>\n",
       "      <td>8.326976</td>\n",
       "      <td>0.953952</td>\n",
       "      <td>largeDoses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14488</td>\n",
       "      <td>7.153469</td>\n",
       "      <td>1.673904</td>\n",
       "      <td>smallDoses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26052</td>\n",
       "      <td>1.441871</td>\n",
       "      <td>0.805124</td>\n",
       "      <td>didntLike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75136</td>\n",
       "      <td>13.147394</td>\n",
       "      <td>0.428964</td>\n",
       "      <td>didntLike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38344</td>\n",
       "      <td>1.669788</td>\n",
       "      <td>0.134296</td>\n",
       "      <td>didntLike</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FrequentFlyerMiles  VideoGameTime  IceCreamConsumed       Label\n",
       "0               40920       8.326976          0.953952  largeDoses\n",
       "1               14488       7.153469          1.673904  smallDoses\n",
       "2               26052       1.441871          0.805124   didntLike\n",
       "3               75136      13.147394          0.428964   didntLike\n",
       "4               38344       1.669788          0.134296   didntLike"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data set and prepare data\n",
    "\n",
    "test_data = pd.read_csv('datingData_test.txt', sep='\\t', header=None)\n",
    "train_data = pd.read_csv('datingData_training.txt',sep = '\\t', header=None)\n",
    "\n",
    "train_data.columns = ['FrequentFlyerMiles','VideoGameTime','IceCreamConsumed','Label']\n",
    "test_data.columns = ['FrequentFlyerMiles','VideoGameTime','IceCreamConsumed','Label']\n",
    "\n",
    "\n",
    "\n",
    "x = train_data.drop(['Label'], axis = 1)\n",
    "y = train_data['Label']\n",
    "\n",
    "x_test = test_data.drop(['Label'], axis = 1)\n",
    "y_test = test_data['Label']\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=None)\n",
    "\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Results\n",
      "\n",
      " Accuracy: 0.9366666666666665, Precision: 0.9435427763502705, F1 Score: 0.9413755325351145\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Model\n",
    "\n",
    "DecisionTreeModel = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=None)\n",
    "\n",
    "# 5-Fold Cross Validation\n",
    "cv_scores_accuracy = cross_val_score(DecisionTreeModel, x, y, cv=10, scoring='accuracy')\n",
    "cv_scores_precision = cross_val_score(DecisionTreeModel, x, y, cv=10, scoring='precision_weighted')\n",
    "cv_scores_f1score = cross_val_score(DecisionTreeModel, x, y, cv=10, scoring='f1_weighted')\n",
    "\n",
    "cv_accuracy_mean = np.mean(cv_scores_accuracy)\n",
    "cv_precision_mean = np.mean(cv_scores_precision)\n",
    "cv_scores_f1 = np.mean(cv_scores_f1score)\n",
    "\n",
    "print(\"Cross Validation Results\")\n",
    "print(f\"\\n Accuracy: {cv_accuracy_mean}, Precision: {cv_precision_mean}, F1 Score: {cv_scores_f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K: 3\n",
      "Accuracy: 0.7816666666666666\n"
     ]
    }
   ],
   "source": [
    "# Nearest Neighbor Model\n",
    "\n",
    "# We will be using grid search cross validation to perform cross validation and to find the best k value for our Model simulataneously.\n",
    "\n",
    "\n",
    "KneighboursModel = KNeighborsClassifier()\n",
    "\n",
    "param_grid = {'n_neighbors': range(1,5)} \n",
    "\n",
    "grid_search = GridSearchCV(KneighboursModel, param_grid, cv=10, scoring='accuracy', return_train_score=False)\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "best_k = grid_search.best_params_['n_neighbors']\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best K: {best_k}\")\n",
    "print(f\"Accuracy: {best_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Results\n",
      "\n",
      " Accuracy: 0.9066666666666666\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machine\n",
    "\n",
    "SVM = SVC(kernel='linear')\n",
    "\n",
    "svm_cv_scores = cross_val_score(SVM, x, y, cv=10, scoring='accuracy')\n",
    "svm_cv_avg = np.mean(svm_cv_scores)\n",
    "print(\"Cross Validation Results\")\n",
    "print(f\"\\n Accuracy: {svm_cv_avg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After evaluating all three models with cross validation we get the following results\n",
    "\n",
    "### Decision Tree Accuracy: 93.6%\n",
    "### Nearest Neighbor Accuracy: 78%\n",
    "### SVM: 90.6%\n",
    "\n",
    "### We will be using our Decision Tree model with parameters max depth of 5 and using gini index as it gives us the highest accuracy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions of model\n",
      "['smallDoses' 'didntLike' 'largeDoses' 'didntLike' 'smallDoses'\n",
      " 'didntLike' 'largeDoses' 'didntLike' 'largeDoses' 'didntLike' 'didntLike'\n",
      " 'smallDoses' 'smallDoses' 'smallDoses' 'largeDoses' 'smallDoses'\n",
      " 'smallDoses' 'didntLike' 'smallDoses' 'largeDoses' 'largeDoses'\n",
      " 'smallDoses' 'largeDoses' 'smallDoses' 'didntLike' 'smallDoses'\n",
      " 'largeDoses' 'largeDoses' 'didntLike' 'largeDoses' 'smallDoses'\n",
      " 'largeDoses' 'smallDoses' 'didntLike' 'smallDoses' 'didntLike'\n",
      " 'didntLike' 'didntLike' 'smallDoses' 'largeDoses' 'smallDoses'\n",
      " 'smallDoses' 'didntLike' 'smallDoses' 'smallDoses' 'didntLike'\n",
      " 'largeDoses' 'didntLike' 'largeDoses' 'largeDoses' 'largeDoses'\n",
      " 'smallDoses' 'smallDoses' 'largeDoses' 'largeDoses' 'didntLike'\n",
      " 'smallDoses' 'smallDoses' 'smallDoses' 'largeDoses' 'didntLike'\n",
      " 'smallDoses' 'didntLike' 'largeDoses' 'didntLike' 'smallDoses'\n",
      " 'largeDoses' 'didntLike' 'didntLike' 'didntLike' 'smallDoses'\n",
      " 'smallDoses' 'largeDoses' 'didntLike' 'largeDoses' 'didntLike'\n",
      " 'didntLike' 'largeDoses' 'didntLike' 'smallDoses' 'largeDoses'\n",
      " 'didntLike' 'smallDoses' 'largeDoses' 'didntLike' 'smallDoses'\n",
      " 'largeDoses' 'smallDoses' 'smallDoses' 'smallDoses' 'largeDoses'\n",
      " 'didntLike' 'largeDoses' 'didntLike' 'smallDoses' 'largeDoses'\n",
      " 'smallDoses' 'smallDoses' 'largeDoses' 'didntLike' 'smallDoses'\n",
      " 'largeDoses' 'smallDoses' 'largeDoses' 'didntLike' 'smallDoses'\n",
      " 'smallDoses' 'largeDoses' 'didntLike' 'didntLike' 'didntLike'\n",
      " 'smallDoses' 'smallDoses' 'didntLike' 'didntLike' 'largeDoses'\n",
      " 'didntLike' 'smallDoses' 'didntLike' 'smallDoses' 'largeDoses'\n",
      " 'smallDoses' 'didntLike' 'largeDoses' 'largeDoses' 'largeDoses'\n",
      " 'didntLike' 'didntLike' 'largeDoses' 'didntLike' 'smallDoses'\n",
      " 'largeDoses' 'largeDoses' 'smallDoses' 'smallDoses' 'smallDoses'\n",
      " 'didntLike' 'smallDoses' 'largeDoses' 'smallDoses' 'smallDoses'\n",
      " 'largeDoses' 'smallDoses' 'smallDoses' 'smallDoses' 'largeDoses'\n",
      " 'largeDoses' 'smallDoses' 'didntLike' 'largeDoses' 'smallDoses'\n",
      " 'didntLike' 'largeDoses' 'largeDoses' 'didntLike' 'smallDoses'\n",
      " 'largeDoses' 'smallDoses' 'didntLike' 'largeDoses' 'largeDoses'\n",
      " 'largeDoses' 'didntLike' 'smallDoses' 'smallDoses' 'smallDoses'\n",
      " 'largeDoses' 'smallDoses' 'largeDoses' 'largeDoses' 'didntLike'\n",
      " 'smallDoses' 'didntLike' 'didntLike' 'smallDoses' 'didntLike'\n",
      " 'largeDoses' 'didntLike' 'smallDoses' 'smallDoses' 'didntLike'\n",
      " 'largeDoses' 'smallDoses' 'didntLike' 'largeDoses' 'largeDoses'\n",
      " 'smallDoses' 'smallDoses' 'smallDoses' 'didntLike' 'smallDoses'\n",
      " 'smallDoses' 'didntLike' 'largeDoses' 'didntLike' 'largeDoses'\n",
      " 'didntLike' 'largeDoses' 'largeDoses' 'didntLike' 'didntLike'\n",
      " 'smallDoses' 'largeDoses' 'smallDoses' 'smallDoses' 'largeDoses'\n",
      " 'didntLike' 'didntLike' 'didntLike' 'didntLike' 'largeDoses' 'smallDoses'\n",
      " 'largeDoses' 'didntLike' 'largeDoses' 'didntLike' 'smallDoses'\n",
      " 'largeDoses' 'didntLike' 'largeDoses' 'didntLike' 'largeDoses'\n",
      " 'didntLike' 'didntLike' 'largeDoses' 'smallDoses' 'largeDoses'\n",
      " 'didntLike' 'didntLike' 'largeDoses' 'largeDoses' 'largeDoses'\n",
      " 'largeDoses' 'didntLike' 'largeDoses' 'smallDoses' 'smallDoses'\n",
      " 'didntLike' 'didntLike' 'largeDoses' 'smallDoses' 'smallDoses'\n",
      " 'didntLike' 'smallDoses' 'didntLike' 'smallDoses' 'didntLike'\n",
      " 'smallDoses' 'didntLike' 'largeDoses' 'smallDoses' 'didntLike'\n",
      " 'smallDoses' 'smallDoses' 'didntLike' 'didntLike' 'smallDoses'\n",
      " 'smallDoses' 'smallDoses' 'didntLike' 'smallDoses' 'didntLike'\n",
      " 'smallDoses' 'didntLike' 'smallDoses' 'largeDoses' 'largeDoses'\n",
      " 'smallDoses' 'largeDoses' 'didntLike' 'didntLike' 'largeDoses'\n",
      " 'largeDoses' 'didntLike' 'smallDoses' 'largeDoses' 'largeDoses'\n",
      " 'smallDoses' 'smallDoses' 'smallDoses' 'didntLike' 'largeDoses'\n",
      " 'largeDoses' 'largeDoses' 'largeDoses' 'largeDoses' 'didntLike'\n",
      " 'didntLike' 'largeDoses' 'smallDoses' 'didntLike' 'smallDoses'\n",
      " 'didntLike' 'smallDoses' 'smallDoses' 'largeDoses' 'smallDoses'\n",
      " 'smallDoses' 'smallDoses' 'largeDoses' 'didntLike' 'smallDoses'\n",
      " 'didntLike' 'smallDoses' 'smallDoses' 'didntLike' 'didntLike'\n",
      " 'smallDoses' 'largeDoses' 'didntLike' 'didntLike' 'didntLike' 'didntLike'\n",
      " 'didntLike' 'largeDoses' 'largeDoses' 'largeDoses' 'largeDoses'\n",
      " 'smallDoses' 'largeDoses' 'didntLike' 'largeDoses' 'largeDoses'\n",
      " 'smallDoses' 'largeDoses' 'smallDoses' 'largeDoses' 'largeDoses'\n",
      " 'smallDoses' 'smallDoses' 'didntLike' 'smallDoses' 'didntLike'\n",
      " 'didntLike' 'largeDoses' 'didntLike' 'didntLike' 'didntLike' 'didntLike'\n",
      " 'largeDoses' 'smallDoses' 'didntLike' 'smallDoses' 'didntLike'\n",
      " 'didntLike' 'smallDoses' 'smallDoses' 'didntLike' 'didntLike' 'didntLike'\n",
      " 'largeDoses' 'didntLike' 'didntLike' 'smallDoses' 'largeDoses'\n",
      " 'smallDoses' 'smallDoses' 'didntLike' 'largeDoses' 'didntLike'\n",
      " 'smallDoses' 'largeDoses' 'didntLike' 'smallDoses' 'smallDoses'\n",
      " 'smallDoses' 'smallDoses' 'largeDoses' 'smallDoses' 'largeDoses'\n",
      " 'largeDoses' 'didntLike' 'largeDoses' 'didntLike' 'smallDoses'\n",
      " 'largeDoses' 'didntLike' 'largeDoses' 'smallDoses' 'smallDoses'\n",
      " 'smallDoses' 'smallDoses' 'smallDoses' 'smallDoses' 'smallDoses'\n",
      " 'smallDoses' 'smallDoses' 'smallDoses' 'smallDoses' 'smallDoses'\n",
      " 'largeDoses' 'smallDoses' 'smallDoses' 'largeDoses' 'smallDoses'\n",
      " 'smallDoses' 'didntLike' 'largeDoses' 'largeDoses' 'largeDoses']\n",
      "\n",
      " Accuracy of the model: 0.96 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating on test data set\n",
    "\n",
    "DecisionTreeModel.fit(x, y)\n",
    "\n",
    "prediction = DecisionTreeModel.predict(x_test)\n",
    "\n",
    "y_actual = test_data['Label']\n",
    "\n",
    "print(\"Predictions of model\")\n",
    "print(prediction)\n",
    "\n",
    "accuracy = accuracy_score(y_actual, prediction)\n",
    "print(f'\\n Accuracy of the model: {accuracy:.2f} %')\n",
    "prediction.size\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
